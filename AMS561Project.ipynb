{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch for NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 35.3605\n",
      "Epoch [20/100], Loss: 3.9449\n",
      "Epoch [30/100], Loss: 3.4583\n",
      "Epoch [40/100], Loss: 3.1855\n",
      "Epoch [50/100], Loss: 2.9353\n",
      "Epoch [60/100], Loss: 2.7047\n",
      "Epoch [70/100], Loss: 2.4922\n",
      "Epoch [80/100], Loss: 2.2964\n",
      "Epoch [90/100], Loss: 2.1160\n",
      "Epoch [100/100], Loss: 1.9498\n",
      "Predicted output: 17.28887939453125\n"
     ]
    }
   ],
   "source": [
    "# X have shape (num_samples, num_features)\n",
    "# Y have shape (num_samples,)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Input data\n",
    "X = torch.tensor([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                  [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                  [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                  [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                  [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                  [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                  [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                  [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                  [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                  [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "Y = torch.tensor([6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                  36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                  66, 69, 72, 75, 78, 81, 84, 87, 90, 93], dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3\n",
    "model = LinearRegression(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X) \n",
    "    loss = criterion(outputs.squeeze(), Y)  # Squeeze the outputs to match Y's shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "input_data = torch.tensor([[0.5, 0.5, 0.5]])\n",
    "predicted_output = model(input_data)\n",
    "print(\"Predicted output:\", predicted_output.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 2787.1592, Val Loss: 3209.3301\n",
      "Epoch [20/1000], Loss: 2425.9045, Val Loss: 2790.7776\n",
      "Epoch [30/1000], Loss: 1900.8405, Val Loss: 2178.1897\n",
      "Epoch [40/1000], Loss: 1278.3715, Val Loss: 1441.7402\n",
      "Epoch [50/1000], Loss: 707.5755, Val Loss: 770.5229\n",
      "Epoch [60/1000], Loss: 337.6225, Val Loss: 368.5299\n",
      "Epoch [70/1000], Loss: 220.2261, Val Loss: 252.0238\n",
      "Epoch [80/1000], Loss: 201.0434, Val Loss: 221.7056\n",
      "Epoch [90/1000], Loss: 157.7103, Val Loss: 160.5136\n",
      "Epoch [100/1000], Loss: 122.7455, Val Loss: 118.8087\n",
      "Epoch [110/1000], Loss: 96.3299, Val Loss: 97.5761\n",
      "Epoch [120/1000], Loss: 77.7260, Val Loss: 78.3159\n",
      "Epoch [130/1000], Loss: 63.7794, Val Loss: 57.6847\n",
      "Epoch [140/1000], Loss: 52.2385, Val Loss: 46.9999\n",
      "Epoch [150/1000], Loss: 42.1384, Val Loss: 39.0013\n",
      "Epoch [160/1000], Loss: 33.4193, Val Loss: 30.5581\n",
      "Epoch [170/1000], Loss: 26.0843, Val Loss: 24.1137\n",
      "Epoch [180/1000], Loss: 19.9914, Val Loss: 18.6713\n",
      "Epoch [190/1000], Loss: 15.1569, Val Loss: 14.6779\n",
      "Epoch [200/1000], Loss: 11.3354, Val Loss: 11.4669\n",
      "Epoch [210/1000], Loss: 8.3465, Val Loss: 8.8500\n",
      "Epoch [220/1000], Loss: 6.0390, Val Loss: 6.6994\n",
      "Epoch [230/1000], Loss: 4.3169, Val Loss: 5.0749\n",
      "Epoch [240/1000], Loss: 3.0745, Val Loss: 3.8794\n",
      "Epoch [250/1000], Loss: 2.2223, Val Loss: 3.0227\n",
      "Epoch [260/1000], Loss: 1.6575, Val Loss: 2.4108\n",
      "Epoch [270/1000], Loss: 1.2946, Val Loss: 1.9852\n",
      "Epoch [280/1000], Loss: 1.0578, Val Loss: 1.6770\n",
      "Epoch [290/1000], Loss: 0.9014, Val Loss: 1.4506\n",
      "Epoch [300/1000], Loss: 0.7968, Val Loss: 1.2800\n",
      "Epoch [310/1000], Loss: 0.7180, Val Loss: 1.1394\n",
      "Epoch [320/1000], Loss: 0.6476, Val Loss: 1.0105\n",
      "Epoch [330/1000], Loss: 0.5844, Val Loss: 0.9004\n",
      "Epoch [340/1000], Loss: 0.5285, Val Loss: 0.8039\n",
      "Epoch [350/1000], Loss: 0.4789, Val Loss: 0.7162\n",
      "Epoch [360/1000], Loss: 0.4343, Val Loss: 0.6394\n",
      "Epoch [370/1000], Loss: 0.3947, Val Loss: 0.5707\n",
      "Epoch [380/1000], Loss: 0.3598, Val Loss: 0.5094\n",
      "Epoch [390/1000], Loss: 0.3290, Val Loss: 0.4553\n",
      "Epoch [400/1000], Loss: 0.3017, Val Loss: 0.4073\n",
      "Epoch [410/1000], Loss: 0.2721, Val Loss: 0.3568\n",
      "Epoch [420/1000], Loss: 0.2444, Val Loss: 0.3122\n",
      "Epoch [430/1000], Loss: 0.2195, Val Loss: 0.2715\n",
      "Epoch [440/1000], Loss: 0.1973, Val Loss: 0.2340\n",
      "Epoch [450/1000], Loss: 0.1776, Val Loss: 0.2012\n",
      "Epoch [460/1000], Loss: 0.1601, Val Loss: 0.1722\n",
      "Epoch [470/1000], Loss: 0.1443, Val Loss: 0.1470\n",
      "Epoch [480/1000], Loss: 0.1303, Val Loss: 0.1251\n",
      "Epoch [490/1000], Loss: 0.1182, Val Loss: 0.1063\n",
      "Epoch [500/1000], Loss: 0.1079, Val Loss: 0.0904\n",
      "Epoch [510/1000], Loss: 0.0991, Val Loss: 0.0768\n",
      "Epoch [520/1000], Loss: 0.0914, Val Loss: 0.0652\n",
      "Epoch [530/1000], Loss: 0.0845, Val Loss: 0.0554\n",
      "Epoch [540/1000], Loss: 0.0785, Val Loss: 0.0471\n",
      "Epoch [550/1000], Loss: 0.0733, Val Loss: 0.0401\n",
      "Epoch [560/1000], Loss: 0.0687, Val Loss: 0.0342\n",
      "Epoch [570/1000], Loss: 0.0648, Val Loss: 0.0293\n",
      "Epoch [580/1000], Loss: 0.0593, Val Loss: 0.0231\n",
      "Epoch [590/1000], Loss: 0.0535, Val Loss: 0.0185\n",
      "Epoch [600/1000], Loss: 0.0483, Val Loss: 0.0142\n",
      "Epoch [610/1000], Loss: 0.0436, Val Loss: 0.0107\n",
      "Epoch [620/1000], Loss: 0.0394, Val Loss: 0.0081\n",
      "Epoch [630/1000], Loss: 0.0356, Val Loss: 0.0064\n",
      "Epoch [640/1000], Loss: 0.0322, Val Loss: 0.0054\n",
      "Epoch [650/1000], Loss: 0.0291, Val Loss: 0.0051\n",
      "Early stopping on epoch 658.\n",
      "Predicted output: 14.616289138793945\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Input data\n",
    "X = torch.tensor([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                  [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                  [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                  [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                  [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                  [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                  [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                  [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                  [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                  [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "Y = torch.tensor([6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                  36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                  66, 69, 72, 75, 78, 81, 84, 87, 90, 93], dtype=torch.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2,random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Define the model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3\n",
    "model = NeuralNet(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = model(torch.tensor(X_train, dtype=torch.float32))\n",
    "    loss = criterion(outputs.squeeze(), Y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "        val_loss = criterion(val_outputs.squeeze(), Y_val)\n",
    "        \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}.')\n",
    "            break\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Test\n",
    "input_data = torch.tensor([[0.5, 0.5, 0.5]])\n",
    "input_data = scaler.transform(input_data) # Normalize\n",
    "predicted_output = model(torch.tensor(input_data, dtype=torch.float32))\n",
    "print(\"Predicted output:\", predicted_output.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand coding NN\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# X = \n",
    "# Y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    \n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The ReLU output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.maximum(0, Z1)  \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = np.maximum(0, Z2)  \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "\n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (A1 >= 0 )\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = copy.deepcopy(parameters[\"b1\"])\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = copy.deepcopy(parameters[\"b2\"])\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        # Cost function\n",
    "        cost = compute_cost(A2, Y)\n",
    "        # Backpropagation\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        # Gradient descent parameter update\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)\n",
    "\n",
    "predictions = predict(parameters, X)\n",
    "print ('Accuracy: %d' % float((np.dot(Y, predictions.T) + np.dot(1 - Y, 1 - predictions.T)) / float(Y.size) * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3192.344634319201\n",
      "Cost after iteration 100: 1961.7756473300826\n",
      "Cost after iteration 200: 1390.8309398811436\n",
      "Cost after iteration 300: 25.643514195660554\n",
      "Cost after iteration 400: 223.4045026294144\n",
      "Cost after iteration 500: 290.93013930640285\n",
      "Cost after iteration 600: 207.72572310576507\n",
      "Cost after iteration 700: 150.46616115280668\n",
      "Cost after iteration 800: 110.31492514153304\n",
      "Cost after iteration 900: 81.6559035499388\n",
      "Predicted output: [[16.71697655]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * np.sqrt(1 / n_x)  # Xavier initialization\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * np.sqrt(1 / n_h)  # Xavier initialization\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = Z2\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = 1/m * np.sum((A2 - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n",
    "    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=1000, print_cost_every=100, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x, _, n_y = layer_sizes(X, Y)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        if print_cost and i % print_cost_every == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return A2\n",
    "\n",
    "# Generate training examples\n",
    "X_train = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                    [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                    [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                    [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                    [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                    [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                    [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                    [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                    [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                    [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "\n",
    "Y_train = np.array([[6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                     36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                     66, 69, 72, 75, 78, 81, 84, 87, 90, 93]])\n",
    "\n",
    "# Train the model\n",
    "parameters = nn_model(X_train.T, Y_train, n_h=4, num_iterations=1000, print_cost_every=100, print_cost=True)\n",
    "\n",
    "# Generate test example\n",
    "input_data = np.array([[0.5, 0.5, 0.5]])\n",
    "\n",
    "# Predict using the trained model\n",
    "predicted_output = predict(parameters, input_data.T)\n",
    "\n",
    "# Print the predicted output\n",
    "print(\"Predicted output:\", predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3040.5402996719863\n",
      "Cost after iteration 100: 1336.891583587331\n",
      "Cost after iteration 200: 256.32575983987067\n",
      "Cost after iteration 300: 373.697447812566\n",
      "Cost after iteration 400: 353.04188598498234\n",
      "Cost after iteration 500: 256.17852345768335\n",
      "Cost after iteration 600: 188.17437346601938\n",
      "Cost after iteration 700: 139.57828031384642\n",
      "Cost after iteration 800: 104.35619967164536\n",
      "Cost after iteration 900: 78.53162555329311\n",
      "Predicted output : [[15.5745637]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * np.sqrt(1 / n_x)\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * np.sqrt(1 / n_h)\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = Z2\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = 1/m * np.sum((A2 - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n",
    "    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.00845):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=1000, print_cost_every=100, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x, _, n_y = layer_sizes(X, Y)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    prev_cost = float('inf') \n",
    "    best_parameters = None\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        if cost < prev_cost:\n",
    "            prev_cost = cost\n",
    "            best_parameters = parameters.copy()\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        if print_cost and i % print_cost_every == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "    return best_parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return A2\n",
    "\n",
    "# Training examples\n",
    "X_train = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                    [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                    [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                    [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                    [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                    [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                    [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                    [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                    [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                    [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "\n",
    "Y_train = np.array([[6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                     36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                     66, 69, 72, 75, 78, 81, 84, 87, 90, 93]])\n",
    "\n",
    "# Test \n",
    "input_data = np.array([[0.5, 0.5, 0.5]])\n",
    "parameters = nn_model(X_train.T, Y_train, n_h=10, num_iterations=1000, print_cost_every=100, print_cost=True)\n",
    "predicted_output = predict(parameters, input_data.T)\n",
    "print(\"Predicted output :\", predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb504e06e19e8d88e2fb0eed19ec0a704236ef277cc2f7218769018cf6791d95"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
