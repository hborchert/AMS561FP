{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using PyTorch for NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 225 entries, 0 to 224\n",
      "Series name: vector\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "225 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 1.9+ KB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '[36.8581052  24.36523118 24.36523118 24.36523118 24.36523118  2.72169106\\n  2.72169106  2.72169106  2.72169106  2.72169106  2.72169106  2.72169106\\n  2.72169106  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.         24.36523118 36.8581052  23.55173203 13.0896349\\n 13.0896349   5.55394942  5.55394942  2.68012094  2.68012094  1.70438946\\n  1.93647644  1.93647644  1.70438946  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.         24.36523118 23.55173203\\n 36.8581052  13.0896349  13.0896349   2.68012094  2.68012094  5.55394942\\n  5.55394942  1.93647644  1.70438946  1.70438946  1.93647644  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n 24.36523118 13.0896349  13.0896349  36.8581052  23.55173203  1.70438946\\n  1.93647644  1.93647644  1.70438946  5.55394942  5.55394942  2.68012094\\n  2.68012094  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.         24.36523118 13.0896349  13.0896349  23.55173203\\n 36.8581052   1.93647644  1.70438946  1.70438946  1.93647644  2.68012094\\n  2.68012094  5.55394942  5.55394942  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          2.72169106  5.55394942\\n  2.68012094  1.70438946  1.93647644  0.5         0.54737354  0.32167482\\n  0.39757068  0.22820251  0.26164576  0.31603478  0.26164576  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  2.72169106  5.55394942  2.68012094  1.93647644  1.70438946  0.54737354\\n  0.5         0.39757068  0.32167482  0.26164576  0.31603478  0.26164576\\n  0.22820251  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          2.72169106  2.68012094  5.55394942  1.93647644\\n  1.70438946  0.32167482  0.39757068  0.5         0.54737354  0.31603478\\n  0.26164576  0.22820251  0.26164576  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          2.72169106  2.68012094\\n  5.55394942  1.70438946  1.93647644  0.39757068  0.32167482  0.54737354\\n  0.5         0.26164576  0.22820251  0.26164576  0.31603478  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  2.72169106  1.70438946  1.93647644  5.55394942  2.68012094  0.22820251\\n  0.26164576  0.31603478  0.26164576  0.5         0.54737354  0.32167482\\n  0.39757068  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          2.72169106  1.93647644  1.70438946  5.55394942\\n  2.68012094  0.26164576  0.31603478  0.26164576  0.22820251  0.54737354\\n  0.5         0.39757068  0.32167482  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          2.72169106  1.93647644\\n  1.70438946  2.68012094  5.55394942  0.31603478  0.26164576  0.22820251\\n  0.26164576  0.32167482  0.39757068  0.5         0.54737354  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  2.72169106  1.70438946  1.93647644  2.68012094  5.55394942  0.26164576\\n  0.22820251  0.26164576  0.31603478  0.39757068  0.32167482  0.54737354\\n  0.5         0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.        ]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m input_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINPUT1_pbe0_lda.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m----> 4\u001b[0m input_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    416\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    417\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    418\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    419\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '[36.8581052  24.36523118 24.36523118 24.36523118 24.36523118  2.72169106\\n  2.72169106  2.72169106  2.72169106  2.72169106  2.72169106  2.72169106\\n  2.72169106  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.         24.36523118 36.8581052  23.55173203 13.0896349\\n 13.0896349   5.55394942  5.55394942  2.68012094  2.68012094  1.70438946\\n  1.93647644  1.93647644  1.70438946  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.         24.36523118 23.55173203\\n 36.8581052  13.0896349  13.0896349   2.68012094  2.68012094  5.55394942\\n  5.55394942  1.93647644  1.70438946  1.70438946  1.93647644  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n 24.36523118 13.0896349  13.0896349  36.8581052  23.55173203  1.70438946\\n  1.93647644  1.93647644  1.70438946  5.55394942  5.55394942  2.68012094\\n  2.68012094  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.         24.36523118 13.0896349  13.0896349  23.55173203\\n 36.8581052   1.93647644  1.70438946  1.70438946  1.93647644  2.68012094\\n  2.68012094  5.55394942  5.55394942  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          2.72169106  5.55394942\\n  2.68012094  1.70438946  1.93647644  0.5         0.54737354  0.32167482\\n  0.39757068  0.22820251  0.26164576  0.31603478  0.26164576  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  2.72169106  5.55394942  2.68012094  1.93647644  1.70438946  0.54737354\\n  0.5         0.39757068  0.32167482  0.26164576  0.31603478  0.26164576\\n  0.22820251  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          2.72169106  2.68012094  5.55394942  1.93647644\\n  1.70438946  0.32167482  0.39757068  0.5         0.54737354  0.31603478\\n  0.26164576  0.22820251  0.26164576  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          2.72169106  2.68012094\\n  5.55394942  1.70438946  1.93647644  0.39757068  0.32167482  0.54737354\\n  0.5         0.26164576  0.22820251  0.26164576  0.31603478  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  2.72169106  1.70438946  1.93647644  5.55394942  2.68012094  0.22820251\\n  0.26164576  0.31603478  0.26164576  0.5         0.54737354  0.32167482\\n  0.39757068  0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          2.72169106  1.93647644  1.70438946  5.55394942\\n  2.68012094  0.26164576  0.31603478  0.26164576  0.22820251  0.54737354\\n  0.5         0.39757068  0.32167482  0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          2.72169106  1.93647644\\n  1.70438946  2.68012094  5.55394942  0.31603478  0.26164576  0.22820251\\n  0.26164576  0.32167482  0.39757068  0.5         0.54737354  0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  2.72169106  1.70438946  1.93647644  2.68012094  5.55394942  0.26164576\\n  0.22820251  0.26164576  0.31603478  0.39757068  0.32167482  0.54737354\\n  0.5         0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.          0.          0.\\n  0.          0.          0.          0.        ]'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_data = pd.read_csv(\"INPUT1_pbe0_lda.csv\")\n",
    "input_data['vector'].info()\n",
    "input_data['vector'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 35.3605\n",
      "Epoch [20/100], Loss: 3.9449\n",
      "Epoch [30/100], Loss: 3.4583\n",
      "Epoch [40/100], Loss: 3.1855\n",
      "Epoch [50/100], Loss: 2.9353\n",
      "Epoch [60/100], Loss: 2.7047\n",
      "Epoch [70/100], Loss: 2.4922\n",
      "Epoch [80/100], Loss: 2.2964\n",
      "Epoch [90/100], Loss: 2.1160\n",
      "Epoch [100/100], Loss: 1.9498\n",
      "Predicted output: 17.28887939453125\n"
     ]
    }
   ],
   "source": [
    "# X have shape (num_samples, num_features)\n",
    "# Y have shape (num_samples,)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Input data\n",
    "X = torch.tensor([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                  [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                  [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                  [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                  [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                  [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                  [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                  [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                  [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                  [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "Y = torch.tensor([6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                  36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                  66, 69, 72, 75, 78, 81, 84, 87, 90, 93], dtype=torch.float32)\n",
    "\n",
    "# Define the model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3\n",
    "model = LinearRegression(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X) \n",
    "    loss = criterion(outputs.squeeze(), Y)  # Squeeze the outputs to match Y's shape\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "input_data = torch.tensor([[0.5, 0.5, 0.5]])\n",
    "predicted_output = model(input_data)\n",
    "print(\"Predicted output:\", predicted_output.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 2743.6184, Val Loss: 3152.5156\n",
      "Epoch [20/1000], Loss: 2330.7668, Val Loss: 2674.5979\n",
      "Epoch [30/1000], Loss: 1742.3226, Val Loss: 1996.2560\n",
      "Epoch [40/1000], Loss: 1075.6017, Val Loss: 1224.0370\n",
      "Epoch [50/1000], Loss: 532.9166, Val Loss: 590.1194\n",
      "Epoch [60/1000], Loss: 273.6700, Val Loss: 296.4165\n",
      "Epoch [70/1000], Loss: 232.0336, Val Loss: 253.7289\n",
      "Epoch [80/1000], Loss: 203.9106, Val Loss: 223.6210\n",
      "Epoch [90/1000], Loss: 154.4285, Val Loss: 173.1462\n",
      "Epoch [100/1000], Loss: 116.5804, Val Loss: 132.4076\n",
      "Epoch [110/1000], Loss: 89.8845, Val Loss: 95.5626\n",
      "Epoch [120/1000], Loss: 70.4576, Val Loss: 67.9636\n",
      "Epoch [130/1000], Loss: 55.7791, Val Loss: 51.8954\n",
      "Epoch [140/1000], Loss: 43.2996, Val Loss: 40.8636\n",
      "Epoch [150/1000], Loss: 33.0741, Val Loss: 32.4834\n",
      "Epoch [160/1000], Loss: 24.9861, Val Loss: 24.8546\n",
      "Epoch [170/1000], Loss: 18.7044, Val Loss: 18.7074\n",
      "Epoch [180/1000], Loss: 13.9454, Val Loss: 14.3232\n",
      "Epoch [190/1000], Loss: 10.4336, Val Loss: 11.1107\n",
      "Epoch [200/1000], Loss: 7.7381, Val Loss: 8.5831\n",
      "Epoch [210/1000], Loss: 5.6793, Val Loss: 6.5052\n",
      "Epoch [220/1000], Loss: 4.1435, Val Loss: 4.9732\n",
      "Epoch [230/1000], Loss: 3.0464, Val Loss: 3.8436\n",
      "Epoch [240/1000], Loss: 2.2818, Val Loss: 3.0312\n",
      "Epoch [250/1000], Loss: 1.7645, Val Loss: 2.4653\n",
      "Epoch [260/1000], Loss: 1.4232, Val Loss: 2.0730\n",
      "Epoch [270/1000], Loss: 1.2021, Val Loss: 1.8003\n",
      "Epoch [280/1000], Loss: 1.0537, Val Loss: 1.6063\n",
      "Epoch [290/1000], Loss: 0.9495, Val Loss: 1.4587\n",
      "Epoch [300/1000], Loss: 0.8729, Val Loss: 1.3450\n",
      "Epoch [310/1000], Loss: 0.8111, Val Loss: 1.2501\n",
      "Epoch [320/1000], Loss: 0.7579, Val Loss: 1.1664\n",
      "Epoch [330/1000], Loss: 0.7112, Val Loss: 1.0923\n",
      "Epoch [340/1000], Loss: 0.6692, Val Loss: 1.0273\n",
      "Epoch [350/1000], Loss: 0.6314, Val Loss: 0.9677\n",
      "Epoch [360/1000], Loss: 0.5966, Val Loss: 0.9120\n",
      "Epoch [370/1000], Loss: 0.5642, Val Loss: 0.8608\n",
      "Epoch [380/1000], Loss: 0.5355, Val Loss: 0.8147\n",
      "Epoch [390/1000], Loss: 0.5094, Val Loss: 0.7718\n",
      "Epoch [400/1000], Loss: 0.4806, Val Loss: 0.7245\n",
      "Epoch [410/1000], Loss: 0.4500, Val Loss: 0.6788\n",
      "Epoch [420/1000], Loss: 0.4211, Val Loss: 0.6356\n",
      "Epoch [430/1000], Loss: 0.3947, Val Loss: 0.5929\n",
      "Epoch [440/1000], Loss: 0.3702, Val Loss: 0.5532\n",
      "Epoch [450/1000], Loss: 0.3473, Val Loss: 0.5123\n",
      "Epoch [460/1000], Loss: 0.3258, Val Loss: 0.4822\n",
      "Epoch [470/1000], Loss: 0.3057, Val Loss: 0.4445\n",
      "Epoch [480/1000], Loss: 0.2874, Val Loss: 0.4168\n",
      "Epoch [490/1000], Loss: 0.2708, Val Loss: 0.3888\n",
      "Epoch [500/1000], Loss: 0.2555, Val Loss: 0.3656\n",
      "Epoch [510/1000], Loss: 0.2412, Val Loss: 0.3402\n",
      "Epoch [520/1000], Loss: 0.2278, Val Loss: 0.3153\n",
      "Epoch [530/1000], Loss: 0.2153, Val Loss: 0.2964\n",
      "Epoch [540/1000], Loss: 0.2038, Val Loss: 0.2792\n",
      "Epoch [550/1000], Loss: 0.1932, Val Loss: 0.2621\n",
      "Epoch [560/1000], Loss: 0.1836, Val Loss: 0.2454\n",
      "Epoch [570/1000], Loss: 0.1747, Val Loss: 0.2322\n",
      "Epoch [580/1000], Loss: 0.1643, Val Loss: 0.2169\n",
      "Epoch [590/1000], Loss: 0.1529, Val Loss: 0.1996\n",
      "Epoch [600/1000], Loss: 0.1420, Val Loss: 0.1844\n",
      "Epoch [610/1000], Loss: 0.1320, Val Loss: 0.1683\n",
      "Epoch [620/1000], Loss: 0.1227, Val Loss: 0.1527\n",
      "Epoch [630/1000], Loss: 0.1146, Val Loss: 0.1413\n",
      "Epoch [640/1000], Loss: 0.1075, Val Loss: 0.1301\n",
      "Epoch [650/1000], Loss: 0.1010, Val Loss: 0.1187\n",
      "Epoch [660/1000], Loss: 0.0952, Val Loss: 0.1090\n",
      "Epoch [670/1000], Loss: 0.0897, Val Loss: 0.0993\n",
      "Epoch [680/1000], Loss: 0.0845, Val Loss: 0.0905\n",
      "Epoch [690/1000], Loss: 0.0798, Val Loss: 0.0819\n",
      "Epoch [700/1000], Loss: 0.0755, Val Loss: 0.0759\n",
      "Epoch [710/1000], Loss: 0.0715, Val Loss: 0.0702\n",
      "Epoch [720/1000], Loss: 0.0680, Val Loss: 0.0647\n",
      "Epoch [730/1000], Loss: 0.0647, Val Loss: 0.0589\n",
      "Epoch [740/1000], Loss: 0.0620, Val Loss: 0.0549\n",
      "Epoch [750/1000], Loss: 0.0597, Val Loss: 0.0516\n",
      "Epoch [760/1000], Loss: 0.0577, Val Loss: 0.0484\n",
      "Epoch [770/1000], Loss: 0.0558, Val Loss: 0.0454\n",
      "Epoch [780/1000], Loss: 0.0541, Val Loss: 0.0423\n",
      "Epoch [790/1000], Loss: 0.0525, Val Loss: 0.0398\n",
      "Epoch [800/1000], Loss: 0.0510, Val Loss: 0.0377\n",
      "Epoch [810/1000], Loss: 0.0497, Val Loss: 0.0359\n",
      "Epoch [820/1000], Loss: 0.0484, Val Loss: 0.0341\n",
      "Epoch [830/1000], Loss: 0.0472, Val Loss: 0.0326\n",
      "Epoch [840/1000], Loss: 0.0460, Val Loss: 0.0310\n",
      "Epoch [850/1000], Loss: 0.0449, Val Loss: 0.0297\n",
      "Epoch [860/1000], Loss: 0.0439, Val Loss: 0.0285\n",
      "Epoch [870/1000], Loss: 0.0430, Val Loss: 0.0275\n",
      "Epoch [880/1000], Loss: 0.0421, Val Loss: 0.0264\n",
      "Epoch [890/1000], Loss: 0.0412, Val Loss: 0.0254\n",
      "Epoch [900/1000], Loss: 0.0404, Val Loss: 0.0245\n",
      "Epoch [910/1000], Loss: 0.0397, Val Loss: 0.0237\n",
      "Epoch [920/1000], Loss: 0.0390, Val Loss: 0.0230\n",
      "Epoch [930/1000], Loss: 0.0384, Val Loss: 0.0224\n",
      "Epoch [940/1000], Loss: 0.0379, Val Loss: 0.0218\n",
      "Epoch [950/1000], Loss: 0.0373, Val Loss: 0.0212\n",
      "Epoch [960/1000], Loss: 0.0367, Val Loss: 0.0207\n",
      "Epoch [970/1000], Loss: 0.0362, Val Loss: 0.0203\n",
      "Epoch [980/1000], Loss: 0.0357, Val Loss: 0.0199\n",
      "Epoch [990/1000], Loss: 0.0352, Val Loss: 0.0196\n",
      "Epoch [1000/1000], Loss: 0.0347, Val Loss: 0.0193\n",
      "Predicted output: 14.949974060058594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Input data\n",
    "X = torch.tensor([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                  [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                  [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                  [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                  [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                  [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                  [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                  [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                  [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                  [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "Y = torch.tensor([6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                  36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                  66, 69, 72, 75, 78, 81, 84, 87, 90, 93], dtype=torch.float32)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2,random_state=42)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Define the model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "input_size = 3\n",
    "model = NeuralNet(input_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "counter = 0\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    outputs = model(torch.tensor(X_train, dtype=torch.float32))\n",
    "    loss = criterion(outputs.squeeze(), Y_train)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation loss\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(torch.tensor(X_val, dtype=torch.float32))\n",
    "        val_loss = criterion(val_outputs.squeeze(), Y_val)\n",
    "        \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}.')\n",
    "            break\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "# Test\n",
    "input_data = torch.tensor([[0.5, 0.5, 0.5]])\n",
    "input_data = scaler.transform(input_data) # Normalize\n",
    "predicted_output = model(torch.tensor(input_data, dtype=torch.float32))\n",
    "print(\"Predicted output:\", predicted_output.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 0.034646165\n",
      "Validation MSE: 0.019315762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def calculate_mse(model, X, Y):\n",
    "    outputs = model(torch.tensor(X, dtype=torch.float32))\n",
    "    loss = mean_squared_error(outputs.squeeze().detach().numpy(), Y)\n",
    "    return loss\n",
    "\n",
    "train_mse = calculate_mse(model, X_train, Y_train)\n",
    "print(\"Training MSE:\", train_mse)\n",
    "\n",
    "val_mse = calculate_mse(model, X_val, Y_val)\n",
    "print(\"Validation MSE:\", val_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand coding NN\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "# X = \n",
    "# Y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    \n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The ReLU output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.maximum(0, Z1)  \n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = np.maximum(0, Z2)  \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)\n",
    "    cost = -1/m * np.sum(logprobs)\n",
    "\n",
    "    cost = float(np.squeeze(cost))\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (A1 >= 0 )\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = copy.deepcopy(parameters[\"b1\"])\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = copy.deepcopy(parameters[\"b2\"])\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        # Cost function\n",
    "        cost = compute_cost(A2, Y)\n",
    "        # Backpropagation\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        # Gradient descent parameter update\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3192.344634319201\n",
      "Cost after iteration 100: 1961.7756473300826\n",
      "Cost after iteration 200: 1390.8309398811436\n",
      "Cost after iteration 300: 25.643514195660554\n",
      "Cost after iteration 400: 223.4045026294144\n",
      "Cost after iteration 500: 290.93013930640285\n",
      "Cost after iteration 600: 207.72572310576507\n",
      "Cost after iteration 700: 150.46616115280668\n",
      "Cost after iteration 800: 110.31492514153304\n",
      "Cost after iteration 900: 81.6559035499388\n",
      "Predicted output: [[16.71697655]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * np.sqrt(1 / n_x)  # Xavier initialization\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * np.sqrt(1 / n_h)  # Xavier initialization\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = Z2\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = 1/m * np.sum((A2 - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n",
    "    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.01):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=1000, print_cost_every=100, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x, _, n_y = layer_sizes(X, Y)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        if print_cost and i % print_cost_every == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "    return parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return A2\n",
    "\n",
    "# Training examples\n",
    "X_train = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                    [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                    [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                    [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                    [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                    [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                    [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                    [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                    [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                    [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "\n",
    "Y_train = np.array([[6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                     36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                     66, 69, 72, 75, 78, 81, 84, 87, 90, 93]])\n",
    "\n",
    "# Test\n",
    "parameters = nn_model(X_train.T, Y_train, n_h=4, num_iterations=1000, print_cost_every=100, print_cost=True)\n",
    "input_data = np.array([[0.5, 0.5, 0.5]])\n",
    "predicted_output = predict(parameters, input_data.T)\n",
    "print(\"Predicted output:\", predicted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3040.5402996719863\n",
      "Cost after iteration 100: 1336.891583587331\n",
      "Cost after iteration 200: 256.32575983987067\n",
      "Cost after iteration 300: 373.697447812566\n",
      "Cost after iteration 400: 353.04188598498234\n",
      "Cost after iteration 500: 256.17852345768335\n",
      "Cost after iteration 600: 188.17437346601938\n",
      "Cost after iteration 700: 139.57828031384642\n",
      "Cost after iteration 800: 104.35619967164536\n",
      "Cost after iteration 900: 78.53162555329311\n",
      "Predicted output : [[15.5745637]]\n",
      "Accuracy: 95.33333333333334 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    n_x = X.shape[0]\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0]\n",
    "    return n_x, n_h, n_y\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(n_h, n_x) * np.sqrt(1 / n_x)\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * np.sqrt(1 / n_h)\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = Z1\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = Z2\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = 1/m * np.sum((A2 - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    m = X.shape[1]\n",
    "    W1, W2 = parameters[\"W1\"], parameters[\"W2\"]\n",
    "    A1, A2 = cache[\"A1\"], cache[\"A2\"]\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = 1/m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2)\n",
    "    dW1 = 1/m * np.dot(dZ1, X.T)\n",
    "    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate=0.00845):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    dW1, db1, dW2, db2 = grads[\"dW1\"], grads[\"db1\"], grads[\"dW2\"], grads[\"db2\"]\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=1000, print_cost_every=100, print_cost=False):\n",
    "    np.random.seed(3)\n",
    "    n_x, _, n_y = layer_sizes(X, Y)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    prev_cost = float('inf') \n",
    "    best_parameters = None\n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        if cost < prev_cost:\n",
    "            prev_cost = cost\n",
    "            best_parameters = parameters.copy()\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        if print_cost and i % print_cost_every == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "    return best_parameters\n",
    "\n",
    "def predict(parameters, X):\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    return A2\n",
    "\n",
    "# Training examples\n",
    "X_train = np.array([[0.1, 0.2, 0.3], [0.2, 0.3, 0.4], [0.3, 0.4, 0.5],\n",
    "                    [0.4, 0.5, 0.6], [0.5, 0.6, 0.7], [0.6, 0.7, 0.8],\n",
    "                    [0.7, 0.8, 0.9], [0.8, 0.9, 1.0], [0.9, 1.0, 1.1],\n",
    "                    [1.0, 1.1, 1.2], [1.1, 1.2, 1.3], [1.2, 1.3, 1.4],\n",
    "                    [1.3, 1.4, 1.5], [1.4, 1.5, 1.6], [1.5, 1.6, 1.7],\n",
    "                    [1.6, 1.7, 1.8], [1.7, 1.8, 1.9], [1.8, 1.9, 2.0],\n",
    "                    [1.9, 2.0, 2.1], [2.0, 2.1, 2.2], [2.1, 2.2, 2.3],\n",
    "                    [2.2, 2.3, 2.4], [2.3, 2.4, 2.5], [2.4, 2.5, 2.6],\n",
    "                    [2.5, 2.6, 2.7], [2.6, 2.7, 2.8], [2.7, 2.8, 2.9],\n",
    "                    [2.8, 2.9, 3.0], [2.9, 3.0, 3.1], [3.0, 3.1, 3.2]])\n",
    "\n",
    "Y_train = np.array([[6, 9, 12, 15, 18, 21, 24, 27, 30, 33,\n",
    "                     36, 39, 42, 45, 48, 51, 54, 57, 60, 63,\n",
    "                     66, 69, 72, 75, 78, 81, 84, 87, 90, 93]])\n",
    "\n",
    "# Test \n",
    "input_data = np.array([[0.5, 0.5, 0.5]])\n",
    "parameters = nn_model(X_train.T, Y_train, n_h=10, num_iterations=1000, print_cost_every=100, print_cost=True)\n",
    "predicted_output = predict(parameters, input_data.T)\n",
    "print(\"Predicted output :\", predicted_output)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error([15], [15.7])\n",
    "accuracy = 1 - mae / 15\n",
    "print('Accuracy:', accuracy * 100, '%')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb504e06e19e8d88e2fb0eed19ec0a704236ef277cc2f7218769018cf6791d95"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
